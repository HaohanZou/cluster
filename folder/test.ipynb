{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "import numpy as np\n",
    "from scipy.optimize import direct, Bounds, shgo\n",
    "from dreal import *\n",
    "\n",
    "import torch\n",
    "import sympytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = sym.symbols(\"x1, x2\")\n",
    "\n",
    "x1_deri = x2\n",
    "x2_deri = -x1 - (1 - x1 ** 2) * x2\n",
    "\n",
    "\n",
    "def final_check(sympy_expr):\n",
    "    x1 = sym.symbols('x1')\n",
    "    x2 = sym.symbols('x2')\n",
    "\n",
    "    x3, x4, x5, x6 = sym.symbols('x3, x4, x5, x6')\n",
    "\n",
    "    x7, x8, x9, x10 = sym.symbols(\"x7, x8, x9, x10\")\n",
    "\n",
    "    origin = float(sympy_expr.evalf(subs = {\"x1\": 0, \"x2\": 0}))\n",
    "    sympy_expr = sympy_expr - origin\n",
    "    # sympy_expr = sympy_expr.subs(x3, 9.81)\n",
    "    numpy_expr = sym.lambdify((x1,x2), sympy_expr, \"numpy\")\n",
    "    v_dot = -1 * (sympy_expr.diff(x1) * x1_deri + sympy_expr.diff(x2) * x2_deri)\n",
    "    numpy_v_dot = sym.lambdify((x1,x2), v_dot, \"numpy\")\n",
    "\n",
    "    \n",
    "    # whole state random check\n",
    "    global function_check_1\n",
    "    global function_check_2\n",
    "\n",
    "    def function_check_1(x):\n",
    "        return numpy_expr(x[0], x[1])\n",
    "    \n",
    "    def function_check_2(x):\n",
    "        return numpy_v_dot(x[0], x[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    num_points = 10 ** 7\n",
    "    # pool_1 = None\n",
    "    for i in range(5):\n",
    "        # data = np.random.uniform(-1.0, 1.0, (1 * 10 ** 7, 6))\n",
    "\n",
    "        boundary_points = []\n",
    "\n",
    "        # Iterate over each dimension to create points near the boundaries\n",
    "        for j in range(2):\n",
    "            # Points near the -1 boundary for the i-th dimension\n",
    "            point_set_low = np.random.uniform(-1.0, 1.0, (num_points, 2))\n",
    "            point_set_low[:, j] = -1 + np.abs(0.001 * np.random.rand(num_points))\n",
    "    \n",
    "            # Points near the 1 boundary for the i-th dimension\n",
    "            point_set_high = np.random.uniform(-1.0, 1.0, (num_points, 2))\n",
    "            point_set_high[:, j] = 1 -  np.abs(0.001 * np.random.rand(num_points))\n",
    "    \n",
    "\n",
    "            point_uniform = np.random.uniform(-1.0, 1.0, (num_points,2))\n",
    "            # Add these points to the boundary_points list\n",
    "            boundary_points.append(point_set_low)\n",
    "            boundary_points.append(point_set_high)\n",
    "            boundary_points.append(point_uniform)\n",
    "\n",
    "        # Combine all boundary points into one array\n",
    "        boundary_points = np.vstack(boundary_points)\n",
    "\n",
    "        check_1 = function_check_1(boundary_points)\n",
    "        check_2 = function_check_2(boundary_points)\n",
    "\n",
    "        if any(check_1 < 0) or any(check_2 < 0):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def pgd_attack(\n",
    "    x0, f, eps, steps=10, lower_boundary=None, upper_boundary=None, direction=\"minimize\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Use adversarial attack (PGD) to find violating points.\n",
    "    Args:\n",
    "      x0: initialization points, in [batch, state_dim].\n",
    "      f: function f(x) to find the worst case x to maximize.\n",
    "      eps: perturbation added to x0.\n",
    "      steps: number of pgd steps.\n",
    "      lower_boundary: absolute lower bounds of x.\n",
    "      upper_boundary: absolute upper bounds of x.\n",
    "    \"\"\"\n",
    "    # Set all parameters without gradient, this can speedup things significantly\n",
    "    grad_status = {}\n",
    "    try:\n",
    "        for p in f.parameters():\n",
    "            grad_status[p] = p.requires_grad\n",
    "            p.requires_grad_(False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    step_size = eps / steps * 2\n",
    "    noise = torch.randn_like(x0) * step_size\n",
    "    if lower_boundary is not None:\n",
    "        lower_boundary = torch.max(lower_boundary, x0 - eps)\n",
    "    else:\n",
    "        lower_boundary = x0 - eps\n",
    "    if upper_boundary is not None:\n",
    "        upper_boundary = torch.min(upper_boundary, x0 + eps)\n",
    "    else:\n",
    "        upper_boundary = x0 + eps\n",
    "    x = x0.detach().clone().requires_grad_()\n",
    "    # Save the best x and best loss.\n",
    "    best_x = torch.clone(x).detach().requires_grad_(False)\n",
    "    fill_value = float(\"-inf\") if direction == \"maximize\" else float(\"inf\")\n",
    "    best_loss = torch.full(\n",
    "        size=(x.size(0),),\n",
    "        requires_grad=False,\n",
    "        fill_value=fill_value,\n",
    "        device=x.device,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    for i in range(steps):\n",
    "        output = f(x1 = x[:,[0]], x2 = x[:,[1]]).squeeze(1).squeeze(1)\n",
    "        # output = torch.clamp(f(x).squeeze(1), max=0)\n",
    "        output.mean().backward()\n",
    "        if direction == \"maximize\":\n",
    "            improved_mask = output >= best_loss\n",
    "        else:\n",
    "            improved_mask = output <= best_loss\n",
    "        best_x[improved_mask] = x[improved_mask]\n",
    "        best_loss[improved_mask] = output[improved_mask]\n",
    "        # print(f'step = {i}', output.view(-1).detach())\n",
    "        # print(x.detach(), best_x)\n",
    "        noise = torch.randn_like(x0) * step_size / (i + 1)\n",
    "        if direction == \"maximize\":\n",
    "            x = (\n",
    "                (\n",
    "                    torch.clamp(\n",
    "                        x + torch.sign(x.grad) * step_size + noise,\n",
    "                        min=lower_boundary,\n",
    "                        max=upper_boundary,\n",
    "                    )\n",
    "                )\n",
    "                .detach()\n",
    "                .requires_grad_()\n",
    "            )\n",
    "        else:\n",
    "            x = (\n",
    "                (\n",
    "                    torch.clamp(\n",
    "                        x - torch.sign(x.grad) * step_size + noise,\n",
    "                        min=lower_boundary,\n",
    "                        max=upper_boundary,\n",
    "                    )\n",
    "                )\n",
    "                .detach()\n",
    "                .requires_grad_()\n",
    "            )\n",
    "\n",
    "    # restore the gradient requirement for model parameters\n",
    "    try:\n",
    "        for p in f.parameters():\n",
    "            p.requires_grad_(grad_status[p])\n",
    "    except:\n",
    "        pass\n",
    "    return best_x\n",
    "\n",
    "def pgd_check(sympy, X):\n",
    "\n",
    "    if len(list(sympy.free_symbols)) < 2:\n",
    "        return False, np.array([])\n",
    "\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = sym.symbols('x1, x2, x3, x4, x5, x6, x7, x8')\n",
    "\n",
    "    sympy = -1 * (sympy.diff(x1) * x1_deri + sympy.diff(x2) * x2_deri)\n",
    "\n",
    "    if len(list(sympy.free_symbols)) < 1:\n",
    "        return False, np.array([])\n",
    "    \n",
    "    sympy_torch = sympytorch.SymPyModule(expressions=[sympy]).to(\"cuda:1\")\n",
    "    data = torch.rand((X.shape[0], X.shape[1])).to(\"cuda:1\")\n",
    "    result = pgd_attack(data, sympy_torch, 0.8, steps=30, lower_boundary=torch.tensor([-1.0, -1.0]).to(\"cuda:1\"), upper_boundary=torch.tensor([1.0, 1.0]).to(\"cuda:1\"), direction=\"minimize\")\n",
    "    result = torch.clamp(result, -1.0, 1.0)\n",
    "    eval = sympy_torch(x1 = result[:,[0]], x2 = result[:,[1]]).squeeze(1).squeeze(1)\n",
    "    result = result[eval < - 0]\n",
    "\n",
    "    return (len(result) == 0), result.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_check(x1 ** 2 + x2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, array([], shape=(0, 2), dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgd_check(x1 ** 2 + x2 ** 2, torch.ones((1000000, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_calculate(v, x1, x2):\n",
    "\n",
    "    \n",
    "    # x1_deri = -x1 + 0.5 * x2 - 0.1 * x5 ** 2\n",
    "    # x2_deri = -0.5 * x1 - x2\n",
    "    # x3_deri = -x3 + 0.5 * x4 - 0.1 * x1 ** 2\n",
    "    # x4_deri = -0.5 * x3 - x4\n",
    "    # x5_deri = -x5 + 0.5 * x6\n",
    "    # x6_deri = -0.5 * x5 - x6 + 0.1 * x2 ** 2\n",
    "\n",
    "    # x7_deri = -x7 + 0.5 * x8\n",
    "    # x8_deri = -0.5 * x7 - x8\n",
    "    \n",
    "\n",
    "    # x1_deri = x2\n",
    "    # x2_deri = - 5 * sym.sin(x1) - 0.1 * x2\n",
    "    \n",
    "    x1_deri = x2\n",
    "    x2_deri = -x1 - (1 - x1 ** 2) * x2\n",
    "\n",
    "    # x1_deri = x2\n",
    "    # x2_deri = - 1 * sym.sin(x1)*sym.cos(x1) - x2 - 1 * sym.sin(x3)*sym.cos(x3)\n",
    "    # x3_deri = x2 - x3\n",
    "\n",
    "    # x1_deri = x2\n",
    "    # x2_deri = - 1 * x1 - x2 - 1 * x3\n",
    "    # x3_deri = x2 - x3\n",
    "\n",
    "    # x1_deri = -x1 + 0.5 * x2 - 0.1 * x3 ** 2\n",
    "    # x2_deri = -0.5 * x1 - x2\n",
    "    # x3_deri = -x3 + 0.5 * x4 - 0.1 * x1 ** 2\n",
    "    # x4_deri = -0.5 * x3 - x4\n",
    "    \n",
    "\n",
    "    \n",
    "    # x1_deri = x2\n",
    "    # x2_deri = - 1 * sym.sin(x1)*sym.cos(x1) - x2 - 1 * sym.sin(x3)*sym.cos(x3)\n",
    "    # x3_deri = x2 - x3\n",
    "\n",
    "    # x1_deri = sym.sin(x2)\n",
    "    # x2_deri = -sym.cos(x2) / (1 - x1)\n",
    "    # x2_deri = -x2 - 1 * (sym.sin(x2) / (x2)) *x1\n",
    "\n",
    "    # x1_deri = -x1 + 0.5 * x2 - 0.1 * x9 ** 2\n",
    "    # x2_deri = -0.5 * x1 - x2\n",
    "    # x3_deri = -x3 + 0.5 * x4 - 0.1 * x1 ** 2\n",
    "    # x4_deri = -0.5 * x3 - x4\n",
    "    # x5_deri = -x5 + 0.5 * x6 + 0.1 * x7 ** 2\n",
    "    # x6_deri = -0.5 * x5 - x6\n",
    "    # x7_deri = -x7 + 0.5 * x8\n",
    "    # x8_deri = -0.5 * x7 - x8\n",
    "    # x9_deri = -x9 + 0.5 * x10\n",
    "    # x10_deri = -0.5 * x9 - x10 + 0.1 * x2 ** 2\n",
    "\n",
    "    # x1_deri = x4 - (x4 + x5 + x6) / 3\n",
    "    # x2_deri = x5 - (x4 + x5 + x6) / 3\n",
    "    # x3_deri = x6 - (x4 + x5 + x6) / 3\n",
    "\n",
    "    # x4_deri = -2 * x4 - sym.sin(x1 - x2) - sym.sin(x1 - x3)\n",
    "    # x5_deri = -2 * x5 - sym.sin(x2 - x1) - sym.sin(x2 - x3)\n",
    "    # x6_deri = -2 * x6 - sym.sin(x3 - x1) - sym.sin(x3 - x2)\n",
    "\n",
    "    # x1_deri = -x1 + 0.5 * x2 - 0.1 * x5 ** 2\n",
    "    # x2_deri = -0.5 * x1 - x2\n",
    "    # x3_deri = -x3 + 0.5 * x4 - 0.1 * x1 ** 2\n",
    "    # x4_deri = -0.5 * x3 - x4\n",
    "    # x5_deri = -x5 + 0.5 * x6 + 0.1 * x7 ** 2\n",
    "    # x6_deri = -0.5 * x5 - x6\n",
    "    # x7_deri = -x7 + 0.5 * x8\n",
    "    # x8_deri = -0.5 * x7 - x8 - 0.1 * x4 ** 2\n",
    "\n",
    "    # x1_deri = -x1 + 0.5 * x2 - 0.1 * x7 ** 2\n",
    "    # x2_deri = -0.5 * x1 - x2\n",
    "    # x3_deri = -x3 + 0.5 * x4 + 0.1 * x5 ** 2\n",
    "    # x4_deri = -0.5 * x3 - x4\n",
    "    # x5_deri = -x5 + 0.5 * x6 \n",
    "    # x6_deri = -0.5 * x5 - x6\n",
    "    # x7_deri = -x7 + 0.5 * x8\n",
    "    # x8_deri = -0.5 * x7 - x8 + 0.1 * x2 ** 2\n",
    "\n",
    "    return ((-1) * (v.diff(x1) * x1_deri + v.diff(x2) * x2_deri))#  + v.diff(x3) * x3_deri + v.diff(x4) * x4_deri+ v.diff(x5) * x5_deri + v.diff(x6) * x6_deri+ v.diff(x7) * x7_deri + v.diff(x8) * x8_deri  # # ))#  # + v.diff(x9) * x9_deri+ v.diff(x10) * x10_deri\n",
    "\n",
    "\n",
    "def find_root(func):\n",
    "    # root = optimize.fsolve(func, guess)\n",
    "    # outer_root = False\n",
    "\n",
    "    bounds = Bounds([-1.0, -1.0], [1.0, 1.0])\n",
    "\n",
    "    result = shgo(func, bounds, n = 1024, iters = 3, sampling_method = \"simplicial\")\n",
    "    root = result.x\n",
    "\n",
    "\n",
    "    if abs(root[0]) > 1.5:\n",
    "        outer_root = True\n",
    "        outer_root = True\n",
    "    \n",
    "    # elif abs(root[2]) > 1:\n",
    "        # outer_root = True\n",
    "    \n",
    "    # elif abs(root[3]) > 1:\n",
    "        # outer_root = True\n",
    "    '''\n",
    "    elif abs(root[4]) > 1:\n",
    "        outer_root = True\n",
    "    elif abs(root[5]) > 1:\n",
    "        outer_root = True\n",
    "    '''\n",
    "        \n",
    "    # res = func(root)\n",
    "    # res_success = (np.abs(res[0])<=0.001 and ((np.linalg.norm(root)>0.001) and (not outer_root)))\n",
    "    # if np.linalg.norm(guess)==0:\n",
    "        # res_success = ((np.abs(res[0])<=0.001 and (np.linalg.norm(root)<0.001)) or (np.abs(res[0])>0.0001))\n",
    "    res_success = True\n",
    "    return root, res_success\n",
    "\n",
    "def counter_exp_finder_deri(root1, func1, root2, func2, num=400):\n",
    "    counter_example = []\n",
    "    pd_counter_example = []\n",
    "    # distance = np.linspace(np.array([-0.5,-0.5, -0.5]),np.array([0.5,0.5, 0.5]),num)\n",
    "    distance = np.random.uniform(0, 0.25, (num,2)) # 0.3\n",
    "    # for i in distance:\n",
    "        # for j in range(len(i)):\n",
    "            # i[j] += np.random.randn()\n",
    "\n",
    "    distance = np.concatenate((distance,np.random.randn(distance.shape[0],distance.shape[1])*0.01),axis=0)\n",
    "    for j in range(num*2):\n",
    "        root1_minus = root1 - distance[j]\n",
    "        root1_minus = np.clip(root1_minus, -1.0, 1.0)\n",
    "        root1_plus = root1 + distance[j]\n",
    "        root1_plus = np.clip(root1_plus, -1.0, 1.0)\n",
    "        root2_minus = root2 - distance[j]\n",
    "        root2_minus = np.clip(root2_minus, -1.0, 1.0)\n",
    "        root2_plus = root2 + distance[j]\n",
    "        root2_plus = np.clip(root2_plus, -1.0, 1.0)\n",
    "\n",
    "        value1 = func1(root1_minus)\n",
    "        value2 = func1(root1_plus)\n",
    "        value3 = func2(root2_minus)\n",
    "        value4 = func2(root2_plus)\n",
    "        \n",
    "        if value1 < 0:\n",
    "            '''\n",
    "            if np.sum([i == 0 for i in root1_minus]):\n",
    "                if value1[0] < 0:\n",
    "                    pd_counter_example.append((root1_minus).copy())\n",
    "                else:\n",
    "                    pd_counter_example.append((root1_minus).copy())\n",
    "            '''\n",
    "            pd_counter_example.append((root1_minus).copy())\n",
    "\n",
    "        if value2 < 0:\n",
    "            '''\n",
    "            if np.sum([i == 0 for i in root1_plus]):\n",
    "                if value1[0] < 0:\n",
    "                    pd_counter_example.append((root1_plus).copy())\n",
    "                else:\n",
    "                    pd_counter_example.append((root1_plus).copy())\n",
    "            '''\n",
    "            pd_counter_example.append((root1_plus).copy())\n",
    "        \n",
    "        if value3 < - 0:\n",
    "            '''\n",
    "            if np.sum([i == 0 for i in root2_minus]):\n",
    "                if value3 < - 0.0001:\n",
    "                    counter_example.append((root2_minus).copy())\n",
    "            else:\n",
    "                if value3[0] < - 0.0001:\n",
    "                    counter_example.append((root2_minus).copy())\n",
    "            '''\n",
    "            counter_example.append((root2_minus).copy())\n",
    "        if value4 < - 0:\n",
    "            '''\n",
    "            if np.sum([i == 0 for i in root2_plus]):\n",
    "                if value4[0] < - 0.0001:\n",
    "                    counter_example.append((root2_plus).copy())\n",
    "            else:\n",
    "                if value4[0] < - 0.0001:\n",
    "                    counter_example.append((root2_plus).copy())\n",
    "            '''\n",
    "            counter_example.append((root2_plus).copy())\n",
    "    del distance\n",
    "\n",
    "    if func1(root1) < 0:\n",
    "        pd_counter_example.append(root1.copy())\n",
    "    if func2(root2) < 0:\n",
    "        counter_example.append((root2.copy()))\n",
    "    return counter_example, pd_counter_example\n",
    "\n",
    "def check_options(sympy_expr):\n",
    "    \n",
    "    x1 = sym.symbols('x1')\n",
    "    x2 = sym.symbols('x2')\n",
    "\n",
    "    x3, x4, x5, x6 = sym.symbols('x3, x4, x5, x6')\n",
    "\n",
    "    x7, x8, x9, x10 = sym.symbols(\"x7, x8, x9, x10\")\n",
    "\n",
    "    origin = float(sympy_expr.evalf(subs = {\"x1\": 0, \"x2\": 0}))\n",
    "    sympy_expr = sympy_expr - origin\n",
    "    # sympy_expr = sympy_expr.subs(x3, 9.81)\n",
    "    numpy_expr = sym.lambdify((x1,x2), sympy_expr, \"numpy\")\n",
    "    v_dot = derivative_calculate(sympy_expr,x1,x2)\n",
    "    numpy_v_dot = sym.lambdify((x1,x2), v_dot, \"numpy\")\n",
    "        \n",
    "    function1 = lambda x: numpy_expr(x[0], x[1])\n",
    "    function2 = lambda x: numpy_v_dot(x[0], x[1])\n",
    "\n",
    "\n",
    "    root_1, root_1_sucs = find_root(function1)\n",
    "    root_2, root_2_sucs = find_root(function2)\n",
    "\n",
    "    counter_example, pd = counter_exp_finder_deri(root_1, function1, root_2, function2)\n",
    "    counter_exp = counter_example\n",
    "    counter_exp = np.array(counter_exp)\n",
    "\n",
    "    \n",
    "    if ((len(counter_exp) + len(pd)) == 0): \n",
    "        valid = True\n",
    "        return valid, counter_exp\n",
    "    else:\n",
    "        valid = False\n",
    "        return valid, counter_exp\n",
    "    \n",
    "\n",
    "def final_check(sympy_expr):\n",
    "    x1 = sym.symbols('x1')\n",
    "    x2 = sym.symbols('x2')\n",
    "\n",
    "    x3, x4, x5, x6 = sym.symbols('x3, x4, x5, x6')\n",
    "\n",
    "    x7, x8, x9, x10 = sym.symbols(\"x7, x8, x9, x10\")\n",
    "\n",
    "    origin = float(sympy_expr.evalf(subs = {\"x1\": 0, \"x2\": 0}))\n",
    "    sympy_expr = sympy_expr - origin\n",
    "    # sympy_expr = sympy_expr.subs(x3, 9.81)\n",
    "    numpy_expr = sym.lambdify((x1,x2), sympy_expr, \"numpy\")\n",
    "    v_dot = derivative_calculate(sympy_expr,x1,x2)\n",
    "    numpy_v_dot = sym.lambdify((x1,x2), v_dot, \"numpy\")\n",
    "\n",
    "    \n",
    "    # whole state random check\n",
    "    global function_check_1\n",
    "    global function_check_2\n",
    "\n",
    "    def function_check_1(x):\n",
    "        return numpy_expr(x[0], x[1])\n",
    "    \n",
    "    def function_check_2(x):\n",
    "        return numpy_v_dot(x[0], x[1])\n",
    "    \n",
    "    # pgd check\n",
    "    pgd_check_result = True\n",
    "    for i in range(5):\n",
    "        valid, counter_exp = pgd_check(sympy_expr, torch.ones((1000000, 2)))\n",
    "        if not valid:\n",
    "            pgd_check_result = False\n",
    "            print(counter_exp)\n",
    "            break\n",
    "    \n",
    "    if not pgd_check_result:\n",
    "        return False\n",
    "    '''\n",
    "    num_points = 10 ** 7\n",
    "    # pool_1 = None\n",
    "    for i in range(5):\n",
    "        # data = np.random.uniform(-1.0, 1.0, (1 * 10 ** 7, 6))\n",
    "\n",
    "        boundary_points = []\n",
    "\n",
    "        # Iterate over each dimension to create points near the boundaries\n",
    "        for j in range(2):\n",
    "            # Points near the -1 boundary for the i-th dimension\n",
    "            point_set_low = np.random.uniform(-1.0, 1.0, (num_points, 2))\n",
    "            point_set_low[:, j] = -1 + np.abs(0.001 * np.random.rand(num_points))\n",
    "    \n",
    "            # Points near the 1 boundary for the i-th dimension\n",
    "            point_set_high = np.random.uniform(-1.0, 1.0, (num_points, 2))\n",
    "            point_set_high[:, j] = 1 -  np.abs(0.001 * np.random.rand(num_points))\n",
    "    \n",
    "\n",
    "            point_uniform = np.random.uniform(-1.0, 1.0, (num_points, 2))\n",
    "            # Add these points to the boundary_points list\n",
    "            boundary_points.append(point_set_low)\n",
    "            boundary_points.append(point_set_high)\n",
    "            boundary_points.append(point_uniform)\n",
    "\n",
    "        # Combine all boundary points into one array\n",
    "        boundary_points = np.vstack(boundary_points)\n",
    "\n",
    "        check_1 = function_check_1(boundary_points)\n",
    "        check_2 = function_check_2(boundary_points)\n",
    "\n",
    "        if any(check_1 < 0) or any(check_2 < 0):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    '''\n",
    "def pgd_attack(\n",
    "    x0, f, eps, steps=10, lower_boundary=None, upper_boundary=None, direction=\"minimize\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Use adversarial attack (PGD) to find violating points.\n",
    "    Args:\n",
    "      x0: initialization points, in [batch, state_dim].\n",
    "      f: function f(x) to find the worst case x to maximize.\n",
    "      eps: perturbation added to x0.\n",
    "      steps: number of pgd steps.\n",
    "      lower_boundary: absolute lower bounds of x.\n",
    "      upper_boundary: absolute upper bounds of x.\n",
    "    \"\"\"\n",
    "    # Set all parameters without gradient, this can speedup things significantly\n",
    "    grad_status = {}\n",
    "    try:\n",
    "        for p in f.parameters():\n",
    "            grad_status[p] = p.requires_grad\n",
    "            p.requires_grad_(False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    step_size = eps / steps * 2\n",
    "    noise = torch.randn_like(x0) * step_size\n",
    "    if lower_boundary is not None:\n",
    "        lower_boundary = torch.max(lower_boundary, x0 - eps)\n",
    "    else:\n",
    "        lower_boundary = x0 - eps\n",
    "    if upper_boundary is not None:\n",
    "        upper_boundary = torch.min(upper_boundary, x0 + eps)\n",
    "    else:\n",
    "        upper_boundary = x0 + eps\n",
    "    x = x0.detach().clone().requires_grad_()\n",
    "    # Save the best x and best loss.\n",
    "    best_x = torch.clone(x).detach().requires_grad_(False)\n",
    "    fill_value = float(\"-inf\") if direction == \"maximize\" else float(\"inf\")\n",
    "    best_loss = torch.full(\n",
    "        size=(x.size(0),),\n",
    "        requires_grad=False,\n",
    "        fill_value=fill_value,\n",
    "        device=x.device,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    for i in range(steps):\n",
    "        output = f(x1 = x[:,[0]], x2 = x[:,[1]]).squeeze(1).squeeze(1)\n",
    "        # output = torch.clamp(f(x).squeeze(1), max=0)\n",
    "        output.mean().backward()\n",
    "        if direction == \"maximize\":\n",
    "            improved_mask = output >= best_loss\n",
    "        else:\n",
    "            improved_mask = output <= best_loss\n",
    "        best_x[improved_mask] = x[improved_mask]\n",
    "        best_loss[improved_mask] = output[improved_mask]\n",
    "        # print(f'step = {i}', output.view(-1).detach())\n",
    "        # print(x.detach(), best_x)\n",
    "        noise = torch.randn_like(x0) * step_size / (i + 1)\n",
    "        if direction == \"maximize\":\n",
    "            x = (\n",
    "                (\n",
    "                    torch.clamp(\n",
    "                        x + torch.sign(x.grad) * step_size + noise,\n",
    "                        min=lower_boundary,\n",
    "                        max=upper_boundary,\n",
    "                    )\n",
    "                )\n",
    "                .detach()\n",
    "                .requires_grad_()\n",
    "            )\n",
    "        else:\n",
    "            x = (\n",
    "                (\n",
    "                    torch.clamp(\n",
    "                        x - torch.sign(x.grad) * step_size + noise,\n",
    "                        min=lower_boundary,\n",
    "                        max=upper_boundary,\n",
    "                    )\n",
    "                )\n",
    "                .detach()\n",
    "                .requires_grad_()\n",
    "            )\n",
    "\n",
    "    # restore the gradient requirement for model parameters\n",
    "    try:\n",
    "        for p in f.parameters():\n",
    "            p.requires_grad_(grad_status[p])\n",
    "    except:\n",
    "        pass\n",
    "    return best_x\n",
    "    \n",
    "\n",
    "def pgd_check(sympy, X):\n",
    "\n",
    "    if len(list(sympy.free_symbols)) < 2:\n",
    "        return False, np.array([])\n",
    "\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = sym.symbols('x1, x2, x3, x4, x5, x6, x7, x8')\n",
    "\n",
    "    sympy = derivative_calculate(sympy, x1, x2)\n",
    "\n",
    "    if len(list(sympy.free_symbols)) < 1:\n",
    "        return False, np.array([])\n",
    "    \n",
    "    sympy_torch = sympytorch.SymPyModule(expressions=[sympy]).to(\"cuda:1\")\n",
    "    data = torch.rand((X.shape[0], X.shape[1])).to(\"cuda:1\")\n",
    "    result = pgd_attack(data, sympy_torch, 0.8, steps=30, lower_boundary=torch.tensor([-1.0, -1.0]).to(\"cuda:1\"), upper_boundary=torch.tensor([1.0, 1.0]).to(\"cuda:1\"), direction=\"minimize\")\n",
    "    result = torch.clamp(result, -1.0, 1.0)\n",
    "    evaluation = sympy_torch(x1 = result[:,[0]], x2 = result[:,[1]]).squeeze(1).squeeze(1)\n",
    "    result = result[evaluation < - 1e-15]\n",
    "    print(sympy_torch(x1 = result[:,[0]], x2 = result[:,[1]]).squeeze(1).squeeze(1))\n",
    "\n",
    "    return (len(result) == 0), result.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, array([], dtype=float64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = sym.symbols(\"x1, x2\")\n",
    "\n",
    "V = (x1 ** 2 + x2 ** 2) ** 8\n",
    "\n",
    "check_options(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], device='cuda:1', grad_fn=<SqueezeBackward1>)\n",
      "tensor([], device='cuda:1', grad_fn=<SqueezeBackward1>)\n",
      "tensor([], device='cuda:1', grad_fn=<SqueezeBackward1>)\n",
      "tensor([], device='cuda:1', grad_fn=<SqueezeBackward1>)\n",
      "tensor([], device='cuda:1', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "final_check(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
